{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCedazVxogHf"
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "In this assignment you will be implementing at least 2 unique models and working with both the CIFAR10 and Imagenette datasets. The cell below defines a basic model based on the examples we reviewed in class. You should use this as a starting point to implement your own models.\n",
    "\n",
    "You should have no problems running this notebook in Google Colab. If you choose to run it on your own machine, a yaml file is provided with the necessary dependencies. If you are using anaconda, you can create a new environment with the following command:\n",
    "\n",
    "```bash\n",
    "conda env create -f env.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YQsyhliuDlI"
   },
   "source": [
    "#Import torch and test cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-Jg8KSM4S4h"
   },
   "source": [
    "#Install Lighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3kwtaNrvsaP"
   },
   "source": [
    "#BaselineModel (MLP) provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "class BaselineModel(L.LightningModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.estimator = nn.Sequential(\n",
    "            nn.Linear(64 * 64, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        return self.estimator(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.accuracy(y_hat, y)\n",
    "\n",
    "        self.log(\"val_accuracy\", self.accuracy)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.accuracy(y_hat, y)\n",
    "\n",
    "        self.log(\"test_accuracy\", self.accuracy)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLcgw-lZxgEM"
   },
   "source": [
    "#Imagenette dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUIrOkWCogHh"
   },
   "source": [
    "The Imagenette dataset is a smaller subset of 10 easily classified classes from Imagenet. It is available to download from `torchvision`, as shown in the cell below. There are 3 different sizes of the images available. Feel free to use whichever version you prefer. It might make a difference in the performance of your model.\n",
    "\n",
    "**Note: After downloading the Imagenette dataset, you will need to set `download=False` in the cell below to avoid errors.(Note: I had no issue without changing to the false.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import Imagenette\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# Prepare the dataset. Without Data augementation.\n",
    "# train_transforms = transforms.Compose([\n",
    "#     transforms.CenterCrop(160),\n",
    "#     transforms.Resize(64),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "#     transforms.Grayscale()\n",
    "# ])\n",
    "\n",
    "# Prepare the dataset for the Regularization. Use this for regularization.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Convert to 1-channel before anything else\n",
    "    transforms.RandomResizedCrop(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize for 1-channel image\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    transforms.Grayscale()\n",
    "])\n",
    "\n",
    "train_dataset = Imagenette(\"data/imagenette/train/\", split=\"train\", size=\"160px\", download=True, transform=train_transforms)\n",
    "\n",
    "# Use 10% of the training set for validation\n",
    "train_set_size = int(len(train_dataset) * 0.9)\n",
    "val_set_size = len(train_dataset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_set_size, val_set_size], generator=seed)\n",
    "val_dataset.dataset.transform = test_transforms\n",
    "\n",
    "# Use DataLoader to load the dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, num_workers=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, num_workers=8, shuffle=False)\n",
    "\n",
    "# Configure the test dataset\n",
    "test_dataset = Imagenette(\"data/imagenette/test/\", split=\"val\", size=\"160px\", download=True, transform=test_transforms)\n",
    "\n",
    "model = BaselineModel()\n",
    "\n",
    "# Add EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "# Configure Checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# trainer = L.Trainer(callbacks=[early_stop_callback, checkpoint_callback])\n",
    "# trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, num_workers=8, shuffle=False)\n",
    "# trainer.test(model=model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyk_Sx4d0mY3"
   },
   "source": [
    "#Basic Convulution Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(L.LightningModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Input: 1x64x64 -> Output: 32x64x64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output: 32x32x32\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # Output: 64x32x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Output: 64x16x16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), # Output: 128x16x16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                  # Output: 128x8x8\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log(\"val_accuracy\", self.accuracy, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log(\"test_accuracy\", self.accuracy)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL9vYhzD-OuK"
   },
   "source": [
    "**Note: When training and evaluating model, sometimes session might crashed likely due to the hardware limitations. In such case select runtime and restart the session and start running cells from the beginning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1udFtYB36lJd"
   },
   "source": [
    "##Train and evaluate Basic CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = BasicCNN()\n",
    "\n",
    "# Create the Trainer with GPU and callbacks\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",            # Automatically use GPU if available\n",
    "    max_epochs=50,                 # Set a reasonable upper limit\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, num_workers=2, shuffle=False)\n",
    "trainer.test(model=model, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd9CcUqpDBvp"
   },
   "source": [
    "#ResNet 18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ResNet18Classifier(L.LightningModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(pretrained=False)\n",
    "\n",
    "        # Since we're using grayscale images, change the input layer from 3 channels to 1\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Modify the final FC layer to match the number of classes\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log(\"val_accuracy\", self.accuracy, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log(\"test_accuracy\", self.accuracy)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seGsVavyDak9"
   },
   "source": [
    "##Train and Evaluate ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "resnet_model = ResNet18Classifier()\n",
    "\n",
    "# Define Trainer and callbacks\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=50,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model=resnet_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, num_workers=2, shuffle=False)\n",
    "trainer.test(model=resnet_model, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsGq9Aq9JGQB"
   },
   "source": [
    "#Regularization for ResNet 18 model using data augmentation technique.\n",
    "**Steps**\n",
    "- Go back to the Imagenette dataset preparation section.\n",
    "- Uncomment the train_transform section for the regularization and comment the regular train_transform above it.\n",
    "- Re-run the cell.\n",
    "- Comback and run the ResNet 18 with augmentation model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOOdJgLHKC_U"
   },
   "source": [
    "##Train and evaluate ResNet 18 model with Data Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "resnet_model_augmented = ResNet18Classifier()\n",
    "\n",
    "# Define Trainer and callbacks\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=50,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model=resnet_model_augmented, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, num_workers=2, shuffle=False)\n",
    "trainer.test(model=resnet_model_augmented, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkvloyxbnJQq"
   },
   "source": [
    "**Notes**\n",
    "- Go back and undo the comment in the previous steps.\n",
    "- Run the dataset cell again.\n",
    "- Open Files in the sidebar of google colab.\n",
    "- Check the content/data/lighting_logs/version_X.\n",
    "- Note the latest one which is for the ResNet18 with augmented model. We will need this later for transfer learning.\n",
    "- Not to view the inside of the version folder, sometimes it takes time to load the files, so leave the downarrow on and wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRK7l26yOEIo"
   },
   "source": [
    "#Transfer Learning with ResNet18 on CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import models\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alwq6vaBBl4f"
   },
   "source": [
    "##CIFAR10 Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10TransferModel(L.LightningModule):\n",
    "    def __init__(self, lr=1e-3, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_loss_epoch = []\n",
    "        self.val_loss_epoch = []\n",
    "\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 10)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        acc = (preds.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_accuracy\", acc, prog_bar=True)\n",
    "        return {\"test_loss\": loss, \"test_accuracy\": acc}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_train = sum(self.train_losses) / len(self.train_losses)\n",
    "        avg_val = sum(self.val_losses) / len(self.val_losses)\n",
    "        self.train_loss_epoch.append(avg_train)\n",
    "        self.val_loss_epoch.append(avg_val)\n",
    "        print(f\"\\nEpoch {self.current_epoch} - Train Loss: {avg_train:.4f}, Val Loss: {avg_val:.4f}\")\n",
    "        self.train_losses.clear()\n",
    "        self.val_losses.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        CIFAR10(root=\"data\", train=True, download=True)\n",
    "        CIFAR10(root=\"data\", train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        dataset = CIFAR10(\"data\", train=True, transform=transform)\n",
    "        test_set = CIFAR10(\"data\", train=False, transform=transform)\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [45000, 5000])\n",
    "        self.test_dataset = test_set\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.batch_size, num_workers=2)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.hparams.batch_size, num_workers=2)\n",
    "\n",
    "\n",
    "def plot_loss(model, title):\n",
    "    epochs = range(len(model.train_loss_epoch))\n",
    "    plt.plot(epochs, model.train_loss_epoch, label=\"Train Loss\")\n",
    "    plt.plot(epochs, model.val_loss_epoch, label=\"Val Loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKXkhZO2CIkv"
   },
   "source": [
    "## Train from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ik3XkOJsEOUg"
   },
   "source": [
    "**Note this model takes time to converge in the first run.It took me 12-15 min.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from Scratch\n",
    "model_scratch = CIFAR10TransferModel()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "    ],\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "trainer.fit(model_scratch)\n",
    "test_results = trainer.test(model_scratch)\n",
    "print(\"Test Accuracy (From Scratch):\", test_results[0][\"test_accuracy\"])\n",
    "plot_loss(model_scratch, \"Training vs Validation Loss (CIFAR10 - Trained from Scratch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N8WCmgfEdZB"
   },
   "source": [
    "##Fine-Tune with Imagenette-Trained Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKD34Dh4FAYQ"
   },
   "source": [
    "**Note: For the pretrained path we should keep track of the ResNet18 with augemented model in the lighting logs and use its version folder and ckpt file inside the checkpoints folder inside it as shown below.The version folder will keep on increasing each time you run a model. If you are running this make sure you remember the version folder from previous ResNet 18 augmented model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune from Imagenette checkpoint\n",
    "checkpoint_path = \"/content/lightning_logs/version_3/checkpoints/epoch=2-step=201.ckpt\"\n",
    "model_finetune = CIFAR10TransferModel.load_from_checkpoint(checkpoint_path)\n",
    "\n",
    "trainer_finetune = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "    ],\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "trainer_finetune.fit(model_finetune)\n",
    "test_results_ft = trainer_finetune.test(model_finetune)\n",
    "print(\"Test Accuracy (Fine-Tuned):\", test_results_ft[0][\"test_accuracy\"])\n",
    "plot_loss(model_finetune, \"Training vs Validation Loss (CIFAR10 - Fine-Tuned from Imagenette)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
